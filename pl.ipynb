{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms, utils\n",
    "from tqdm import tqdm\n",
    "from util import *\n",
    "\n",
    "\n",
    "from e4e_projection_pl import projection as e4e_projection\n",
    "from model import Discriminator, Generator\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import *\n",
    "\n",
    "from argparse import ArgumentParser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미 준비가 되었다면 실행할 필요 없음\n",
    "'''\n",
    "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
    "!bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2\n",
    "!mv shape_predictor_68_face_landmarks.dat models/dlibshape_predictor_68_face_landmarks.dat\n",
    "\n",
    "# Download pretrained weight\n",
    "!gdown --id 1o6ijA3PkcewZvwJJ73dJ0fxhndn0nnh7 -O /home/aiteam/tykim/JoJoGAN/models/e4e_ffhq_encode.pt\n",
    "\n",
    "!gdown --id 1Yr7KuD959btpmcKGAUsbAk5rPjX2MytK -O /home/aiteam/tykim/JoJoGAN/models/stylegan2-ffhq-config-f.pt\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor((1,3,3)).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, tfms, style_path):\n",
    "    \"\"\"\n",
    "    style_path : style images가 있는 폴더의 path\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    style_imgs_list = os.listdir(style_path)\n",
    "    # con_imgs_list = listdir(con_path)\n",
    "\n",
    "    targets = []\n",
    "    latents = []\n",
    "\n",
    "    for name in style_imgs_list:\n",
    "      style_img_path = os.path.join('style_images', name)\n",
    "      # Style Image를 얻고 이를 Crop and align\n",
    "      # 이미 align한게 있다면 그냥 불러오고 아니면 align_face를 수행한다\n",
    "      name = strip_path_extension(name)\n",
    "      style_aligned_path = os.path.join('style_images_aligned', f'{name}.png')\n",
    "\n",
    "      if os.path.exists(style_aligned_path):\n",
    "        style_aligned = Image.open(style_aligned_path).convert('RGB')\n",
    "      else:\n",
    "        # Alignment를 하고 PIL 이미지를 리턴\n",
    "        style_aligned = align_face(style_img_path)\n",
    "        # png로 저장한다.\n",
    "        style_aligned.save(style_aligned_path)\n",
    "\n",
    "      # w를 찾아낸 뒤 pt로 저장한다.\n",
    "      style_code_path = os.path.join('inversion_codes', f'{name}.pt')\n",
    "      if not os.path.exists(style_code_path):\n",
    "        latent = e4e_projection(style_aligned, style_code_path)\n",
    "      else:\n",
    "        latent = torch.load(style_code_path)['latent']\n",
    "\n",
    "      targets.append(tfms(style_aligned))\n",
    "      latents.append(latent)\n",
    "\n",
    "    targets = torch.stack(targets, 0)\n",
    "    latents = torch.stack(latents, 0) # shape : [the num of styles, 18, 512]\n",
    "    \n",
    "\n",
    "    self.sample = {\"style_targets\" : targets ,\n",
    "                   \"style_latents\": latents}\n",
    "    \n",
    "  def __len__(self): return 1\n",
    "  def __getitem__(self, idx): return self.sample\n",
    "\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "  def __init__(self, style_path):\n",
    "    \n",
    "    super().__init__()\n",
    "    self.style_path = style_path\n",
    "    # self.con_path = con_path\n",
    "    self.transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((1024, 1024)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "  def prepare_data(self):\n",
    "    pass\n",
    "  \n",
    "  def setup(self, stage=None):\n",
    "    if stage == 'fit' or stage == 'None':\n",
    "      self.train = CustomDataset(self.transform, self.style_path)\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return DataLoader(self.train, batch_size=1, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DataModule(style_path='/home/aiteam/tykim/JoJoGAN/style_images')\n",
    "dm.prepare_data()\n",
    "dm.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dm.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JJGAN(pl.LightningModule):\n",
    "  def __init__(self, style_latents, style_targets, alpha):\n",
    "    super().__init__()\n",
    "    self.latent_dim = 512\n",
    "    self.alpha = alpha\n",
    "    # 몇 번째 레이어부터 어디까지 값을 바꿀지 설정\n",
    "    self.id_swap = list(range(7, self.generator.n_latent))\n",
    "    self.latents = style_latents\n",
    "    self.targets = style_targets\n",
    "    self.alpha = alpha\n",
    "    \n",
    "    # 원래 FFHQ에 학습된 generator\n",
    "    self.original_generator = Generator(1024, self.latent_dim, 8, 2)\n",
    "    ckpt = torch.load('models/stylegan2-ffhq-config-f.pt', map_location=lambda storage, loc: storage)\n",
    "    self.original_generator.load_state_dict(ckpt[\"g_ema\"], strict=False)\n",
    "    \n",
    "    # Fine tuned되어질 Generator\n",
    "    self.generator = deepcopy(self.original_generator)\n",
    "    \n",
    "    # Discriminator\n",
    "    self.discriminator = Discriminator(1024, 2).eval()\n",
    "    self.discriminator.load_state_dict(ckpt[\"d\"], strict=False)\n",
    "\n",
    "    # 10000개의 w를 만들고 평균을 낸다. [1,512]\n",
    "    mean_latent = self.original_generator.mean_latent(10000)\n",
    "    \n",
    "  def configure_optimizers(self):\n",
    "    optim.Adam(self.generator, lr=2e-3, betas=(0, 0.99))\n",
    "    \n",
    "  def training_step(self, batch, batch_idx):\n",
    "    # W space를 활용\n",
    "    mean_w = self.generator.get_latent(torch.randn([self.latents.size(0), self.latent_dim])).unsqueeze(1).repreat(1, self.generator.n_latent, 1)\n",
    "\n",
    "    # Style refs에 대한 w를 복사해둠\n",
    "    in_latent = self.latents.clone()\n",
    "    in_latent[: self.id_swap] = self.alpha * self.latents[:, self.id_swap] + (1 - self.alpha) * mean_w[:, self.id_swap]\n",
    "\n",
    "    img = self.generator(in_latent, input_is_latent=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      real_feat = self.discriminator(self.targets) \n",
    "    fake_feat = self.discriminator(img)\n",
    "\n",
    "    loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)\n",
    "    return loss\n",
    "  \n",
    "    \n",
    "  def inference(self, my_w):\n",
    "    \"\"\"\n",
    "    Returns inversion, styled_img\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "      # Inversion\n",
    "      inversion = self.style_generator(my_w, input_is_latent=True)\n",
    "      # Domain styled imgs\n",
    "      my_sample = self.generator(my_w, input_is_latent=True)\n",
    "\n",
    "    return inversion, my_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = ArgumentParser()\n",
    "  parser.add_argument('--exp', type=str)\n",
    "  parser.add_argument('--seed', type=int, default=3000)\n",
    "  \n",
    "  parser.add_argument('--iter', type=int, default=1000)\n",
    "  parser.add_argument('--gpus', type=int, narngs='+')\n",
    "  parser.add_argument('--strategy', type=str, default=None)\n",
    "  parser.add_argument('--nodes', type=int, default=1)\n",
    "  parser.add_argument('--num_workers', type=int, default=int(os.cpu_count()/2))\n",
    "  parser.add_argument('--precision', type=str, default='float32') #float32, float16, bf16\n",
    "  \n",
    "  parser.add_argument('--id_swap', type=int, default=7)\n",
    "  parser.add_argument('--latent_dim', type=int, default=512)\n",
    "  parser.add_argument('--alpha', type=float, default=1.0)\n",
    "\n",
    "  parser.add_argument('--style_path', type=str, defualt='./style_images')\n",
    "  parser.add_argument('--con_path', tpye=str, default='./test_input')\n",
    "  \n",
    "  \n",
    "  args = parser.parse_args()\n",
    "  \n",
    "  os.makedirs('inversion_codes', exist_ok=True)\n",
    "  os.makedirs('style_images', exist_ok=True)\n",
    "  os.makedirs('style_images_aligned', exist_ok=True)\n",
    "  os.makedirs('models', exist_ok=True)\n",
    "  \n",
    "  # shape_predictor 있는지 체크\n",
    "  # 없으면 다운\n",
    "  if not os.path.exists('./models/dlibshape_predictor_68_face_landmarks.dat'):\n",
    "    os.system('wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2')\n",
    "    os.system('bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2')\n",
    "    os.system('mv shape_predictor_68_face_landmarks.dat models/dlibshape_predictor_68_face_landmarks.dat')\n",
    "    \n",
    "  # pretrained weight 있는지 체크\n",
    "  # 없으면 다운\n",
    "  if not os.path.exists('./models/e4e_ffhq_encode.pt'):\n",
    "    os.systme('gdown --id 1o6ijA3PkcewZvwJJ73dJ0fxhndn0nnh7 -O /home/aiteam/tykim/JoJoGAN/models/e4e_ffhq_encode.pt')\n",
    "    \n",
    "  # pretrained weight 있는지 체크\n",
    "  # 없으면 다운\n",
    "  if not os.path.exists('./models/stylegan2-ffhq-config-f.pt'):\n",
    "    os.systme('gdown --id 1Yr7KuD959btpmcKGAUsbAk5rPjX2MytK -O /home/aiteam/tykim/JoJoGAN/models/stylegan2-ffhq-config-f.pt')\n",
    "    \n",
    "  \n",
    "  torch.manual_seed(args.seed)\n",
    "  torch.backends.cudnn.benchmark = True\n",
    "\n",
    "  # PL setup\n",
    "  dm = DataModule(style_path=args.style_path)\n",
    "  dm.prepare_data()\n",
    "  dm.setup(\"fit\")\n",
    "  \n",
    "  jjGan = JJGAN(args)\n",
    "  \n",
    "  tpgb_cb = TQDMProgressBar(refresh_rate=10)\n",
    "\n",
    "  if args.precision == 'float32':\n",
    "    args.precision = 32\n",
    "  elif args.precision == 'float16':\n",
    "    args.precision = 16\n",
    "\n",
    "  \n",
    "  trainer = Trainer(max_epochs=args.iter, gpus=args.gpus, strategy=args.strategy,\n",
    "                    num_nodes=args.nodes, precision=args.precision, callabacks=[tpgb_cb])\n",
    "  trainer.fit(jjGan, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Infernece\n",
    "import os\n",
    "from util import *\n",
    "\n",
    "from e4e_projection import projection as e4e_projection\n",
    "from model import Discriminator, Generator\n",
    "from argparse import ArgumentParser \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  \n",
    "  parser = ArgumentParser()\n",
    "  parser.add_argument('--data_path', type=int, default=1000)\n",
    "  parser.add_argument('--ckpt', type=str)\n",
    "  args = parser.parse_args()\n",
    "  \n",
    "  device = 'cuda'\n",
    "  \n",
    "  con_imgs_list = os.listdir(args.data_path)\n",
    "  \n",
    "  style_generator = Generator(1024, 512, 8, 2).to(device)\n",
    "  ckpt = torch.load(args.ckpt, map_location=lambda storage, loc: storage)\n",
    "  style_generator.load_state_dict(ckpt, strict=False)\n",
    "  \n",
    "  for con_img in con_imgs_list:\n",
    "    filepath = os.path.join(args.data_path, con_img)\n",
    "    name = strip_path_extension() + '.pt'\n",
    "    aligned_face = align_face(filepath)\n",
    "    my_w = e4e_projection(aligned_face, name, device).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      my_sample = style_generator(my_w, input_is_latent=True)\n",
    "      \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sketch3.jpeg',\n",
       " 'sketch5.jpg',\n",
       " 'image_03894.jpg',\n",
       " 'sketch4.jpeg',\n",
       " 'sketch.jpeg',\n",
       " 'sketch2.jpeg']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "listdir('/home/aiteam/tykim/JoJoGAN/style_images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aiteam/tykim/JoJoGAN/style_images/sketch5.jpg\n",
      "/home/aiteam/tykim/JoJoGAN/style_images/image_03894.jpg\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "for name in glob.glob('/home/aiteam/tykim/JoJoGAN/style_images/*.jpg'):\n",
    "  print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from model import Discriminator, Generator\n",
    "\n",
    "original_generator = Generator(1024, 512, 8, 2)\n",
    "ckpt = torch.load('models/stylegan2-ffhq-config-f.pt', map_location=lambda storage, loc: storage)\n",
    "original_generator.load_state_dict(ckpt[\"g_ema\"], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 18, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_generator.get_latent(torch.randn([4, 512])).unsqueeze(1).repeat(1, original_generator.n_latent, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_generator.n_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(original_generator.state_dict(), 'models/test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "t = torch.load('models/test.pt', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Generator(1024, 512, 8, 2).load_state_dict(t, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['style.1.weight', 'style.1.bias', 'style.2.weight', 'style.2.bias', 'style.3.weight', 'style.3.bias', 'style.4.weight', 'style.4.bias', 'style.5.weight', 'style.5.bias', 'style.6.weight', 'style.6.bias', 'style.7.weight', 'style.7.bias', 'style.8.weight', 'style.8.bias', 'input.input', 'conv1.conv.weight', 'conv1.conv.modulation.weight', 'conv1.conv.modulation.bias', 'conv1.noise.weight', 'conv1.activate.bias', 'to_rgb1.bias', 'to_rgb1.conv.weight', 'to_rgb1.conv.modulation.weight', 'to_rgb1.conv.modulation.bias', 'convs.0.conv.weight', 'convs.0.conv.blur.kernel', 'convs.0.conv.modulation.weight', 'convs.0.conv.modulation.bias', 'convs.0.noise.weight', 'convs.0.activate.bias', 'convs.1.conv.weight', 'convs.1.conv.modulation.weight', 'convs.1.conv.modulation.bias', 'convs.1.noise.weight', 'convs.1.activate.bias', 'convs.2.conv.weight', 'convs.2.conv.blur.kernel', 'convs.2.conv.modulation.weight', 'convs.2.conv.modulation.bias', 'convs.2.noise.weight', 'convs.2.activate.bias', 'convs.3.conv.weight', 'convs.3.conv.modulation.weight', 'convs.3.conv.modulation.bias', 'convs.3.noise.weight', 'convs.3.activate.bias', 'convs.4.conv.weight', 'convs.4.conv.blur.kernel', 'convs.4.conv.modulation.weight', 'convs.4.conv.modulation.bias', 'convs.4.noise.weight', 'convs.4.activate.bias', 'convs.5.conv.weight', 'convs.5.conv.modulation.weight', 'convs.5.conv.modulation.bias', 'convs.5.noise.weight', 'convs.5.activate.bias', 'convs.6.conv.weight', 'convs.6.conv.blur.kernel', 'convs.6.conv.modulation.weight', 'convs.6.conv.modulation.bias', 'convs.6.noise.weight', 'convs.6.activate.bias', 'convs.7.conv.weight', 'convs.7.conv.modulation.weight', 'convs.7.conv.modulation.bias', 'convs.7.noise.weight', 'convs.7.activate.bias', 'convs.8.conv.weight', 'convs.8.conv.blur.kernel', 'convs.8.conv.modulation.weight', 'convs.8.conv.modulation.bias', 'convs.8.noise.weight', 'convs.8.activate.bias', 'convs.9.conv.weight', 'convs.9.conv.modulation.weight', 'convs.9.conv.modulation.bias', 'convs.9.noise.weight', 'convs.9.activate.bias', 'convs.10.conv.weight', 'convs.10.conv.blur.kernel', 'convs.10.conv.modulation.weight', 'convs.10.conv.modulation.bias', 'convs.10.noise.weight', 'convs.10.activate.bias', 'convs.11.conv.weight', 'convs.11.conv.modulation.weight', 'convs.11.conv.modulation.bias', 'convs.11.noise.weight', 'convs.11.activate.bias', 'convs.12.conv.weight', 'convs.12.conv.blur.kernel', 'convs.12.conv.modulation.weight', 'convs.12.conv.modulation.bias', 'convs.12.noise.weight', 'convs.12.activate.bias', 'convs.13.conv.weight', 'convs.13.conv.modulation.weight', 'convs.13.conv.modulation.bias', 'convs.13.noise.weight', 'convs.13.activate.bias', 'convs.14.conv.weight', 'convs.14.conv.blur.kernel', 'convs.14.conv.modulation.weight', 'convs.14.conv.modulation.bias', 'convs.14.noise.weight', 'convs.14.activate.bias', 'convs.15.conv.weight', 'convs.15.conv.modulation.weight', 'convs.15.conv.modulation.bias', 'convs.15.noise.weight', 'convs.15.activate.bias', 'to_rgbs.0.bias', 'to_rgbs.0.upsample.kernel', 'to_rgbs.0.conv.weight', 'to_rgbs.0.conv.modulation.weight', 'to_rgbs.0.conv.modulation.bias', 'to_rgbs.1.bias', 'to_rgbs.1.upsample.kernel', 'to_rgbs.1.conv.weight', 'to_rgbs.1.conv.modulation.weight', 'to_rgbs.1.conv.modulation.bias', 'to_rgbs.2.bias', 'to_rgbs.2.upsample.kernel', 'to_rgbs.2.conv.weight', 'to_rgbs.2.conv.modulation.weight', 'to_rgbs.2.conv.modulation.bias', 'to_rgbs.3.bias', 'to_rgbs.3.upsample.kernel', 'to_rgbs.3.conv.weight', 'to_rgbs.3.conv.modulation.weight', 'to_rgbs.3.conv.modulation.bias', 'to_rgbs.4.bias', 'to_rgbs.4.upsample.kernel', 'to_rgbs.4.conv.weight', 'to_rgbs.4.conv.modulation.weight', 'to_rgbs.4.conv.modulation.bias', 'to_rgbs.5.bias', 'to_rgbs.5.upsample.kernel', 'to_rgbs.5.conv.weight', 'to_rgbs.5.conv.modulation.weight', 'to_rgbs.5.conv.modulation.bias', 'to_rgbs.6.bias', 'to_rgbs.6.upsample.kernel', 'to_rgbs.6.conv.weight', 'to_rgbs.6.conv.modulation.weight', 'to_rgbs.6.conv.modulation.bias', 'to_rgbs.7.bias', 'to_rgbs.7.upsample.kernel', 'to_rgbs.7.conv.weight', 'to_rgbs.7.conv.modulation.weight', 'to_rgbs.7.conv.modulation.bias', 'noises.noise_0', 'noises.noise_1', 'noises.noise_2', 'noises.noise_3', 'noises.noise_4', 'noises.noise_5', 'noises.noise_6', 'noises.noise_7', 'noises.noise_8', 'noises.noise_9', 'noises.noise_10', 'noises.noise_11', 'noises.noise_12', 'noises.noise_13', 'noises.noise_14', 'noises.noise_15', 'noises.noise_16'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # style reference latents\n",
    "    self.latents = self.set_style_latents(names)\n",
    "    \n",
    "    \n",
    "  def set_style_latents(self, names):\n",
    "    for name in names:\n",
    "      # Style Image를 얻고 이를 Crop and align\n",
    "      style_path = os.path.join('style_images', name)\n",
    "      assert os.path.exists(style_path), f\"{style_path} does not exist!\"\n",
    "      # Alignment를 하고 png로 저장한다.\n",
    "      name = strip_path_extension(name)\n",
    "      style_aligned_path = os.path.join('style_images_aligned', f'{name}.png')\n",
    "      # 이미 align한게 있다면 그냥 불러온다. \n",
    "      if not os.path.exists(style_aligned_path):\n",
    "        style_aligned = align_face(style_path)\n",
    "        style_aligned.save(style_aligned_path)\n",
    "      else:\n",
    "        style_aligned = Image.open(style_aligned_path).convert('RGB')\n",
    "\n",
    "      # GAN Invert를 해서 w를 찾아낸다.\n",
    "      style_code_path = os.path.join('inversion_codes', f'{name}.pt')\n",
    "      if not os.path.exists(style_code_path):\n",
    "        latent = e4e_projection(style_aligned, style_code_path, device)\n",
    "      else:\n",
    "        latent = torch.load(style_code_path)['latent']\n",
    "\n",
    "      # PIL -> Tensor\n",
    "      # 원래는 여러개가 들어감 [N, 18, 512] 이렇게 되야 하는데 \n",
    "      targets.append(transform(style_aligned).to(device))\n",
    "      latents.append(latent.to(device))\n",
    "\n",
    "      targets = torch.stack(targets, 0)\n",
    "      latents = torch.stack(latents, 0)\n",
    "      return latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in style_imgs_list:\n",
    "      style_img_path = os.path.join('style_images', name)\n",
    "      # Style Image를 얻고 이를 Crop and align\n",
    "      # 이미 align한게 있다면 그냥 불러오고 아니면 align_face를 수행한다\n",
    "      name = strip_path_extension(name)\n",
    "      style_aligned_path = os.path.join('style_images_aligned', f'{name}.png')\n",
    "\n",
    "      if os.path.exists(style_aligned_path):\n",
    "        style_aligned = Image.open(style_aligned_path).convert('RGB')\n",
    "      else:\n",
    "        # Alignment를 하고 PIL 이미지를 리턴\n",
    "        style_aligned = align_face(style_img_path)\n",
    "        # png로 저장한다.\n",
    "        style_aligned.save(style_aligned_path)\n",
    "\n",
    "      # w를 찾아낸 뒤 pt로 저장한다.\n",
    "      style_code_path = os.path.join('inversion_codes', f'{name}.pt')\n",
    "      if not os.path.exists(style_code_path):\n",
    "        latent = e4e_projection(style_aligned, style_code_path)\n",
    "      else:\n",
    "        latent = torch.load(style_code_path)['latent']\n",
    "\n",
    "      targets.append(tfms(style_aligned))\n",
    "      latents.append(latent)\n",
    "\n",
    "    targets = torch.stack(targets, 0)\n",
    "    latents = torch.stack(latents, 0) # shape : [the num of styles, 18, 512]\n",
    "    \n",
    "\n",
    "    self.sample = {\"style_targets\" : targets ,\n",
    "                   \"style_latents\": latents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@ torch.no_grad()\n",
    "def projection(img, name):\n",
    "    model_path = 'models/e4e_ffhq_encode.pt'\n",
    "    ckpt = torch.load(model_path)# , map_location='cpu')\n",
    "    opts = ckpt['opts']\n",
    "    opts['checkpoint_path'] = model_path\n",
    "    opts= Namespace(**opts)\n",
    "    net = pSp(opts).eval()\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(256),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    images, w_plus = net(img, randomize_noise=False, return_latents=True)\n",
    "    result_file = {}\n",
    "    result_file['latent'] = w_plus[0]\n",
    "    torch.save(result_file, name)\n",
    "    return w_plus[0]\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57b94829e446fb13a451e34047536d3c47fb237aad0762e494d982b1e848cca8"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('jj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
